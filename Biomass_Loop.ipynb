{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Biomass - Loop through raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived from Gader, P. , 2020. “Calculate Vegetation Biomass from LiDAR Data in Python” https://www.neonscience.org/resources/learning-hub/tutorials/calc-biomass-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "from osgeo import gdal, osr\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "import rasterio as rio\n",
    "import glob\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings for loop\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#Import biomass specific libraries\n",
    "from skimage.morphology import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.measure import regionprops\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from skimage.segmentation import watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the file of training data  \n",
    "training_data_file = '/home/jovyan/NEON_CO2_Macrosystems_LIDAR/Code/Data/SJER_Biomass_Training.csv'\n",
    "\n",
    "#Read in the training data from a CSV file\n",
    "training_data = np.genfromtxt(training_data_file,delimiter=',') \n",
    "\n",
    "#Grab the biomass (Y) from the first line\n",
    "biomass = training_data[:,0]\n",
    "\n",
    "#Grab the biomass prdeictors from the remaining lines\n",
    "biomass_predictors = training_data[:,1:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=30, random_state=2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define paraemters for Random forest regressor\n",
    "max_depth = 30\n",
    "\n",
    "#Define regressor rules\n",
    "regr_rf = RandomForestRegressor(max_depth=max_depth, random_state=2)\n",
    "\n",
    "#Fit the biomass to regressor variables\n",
    "regr_rf.fit(biomass_predictors,biomass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=30, min_impurity_split=1e-07, n_estimators=10,\n",
       "                      n_jobs=1, random_state=2)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Further define regressor\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=10, n_jobs=1, oob_score=False, random_state=2,\n",
    "           verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File for testing function\n",
    "test = '/home/jovyan/NEON_CO2_Macrosystems_LIDAR/Code/Data/DP3.30015.001/2019_WREF_3/CanopyHeightModelGtif/NEON_D16_WREF_DP3_573000_5071000_CHM.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to open file and process \n",
    "def open_process(chm_file):\n",
    "    chm_dataset = gdal.Open(chm_file)\n",
    "    #Get the raster band object\n",
    "    chm_raster = chm_dataset.GetRasterBand(1)\n",
    "    #Get the NO DATA value\n",
    "    noDataVal_chm = chm_raster.GetNoDataValue()\n",
    "    #Get required metadata from CHM file\n",
    "    cols_chm = chm_dataset.RasterXSize\n",
    "    rows_chm = chm_dataset.RasterYSize\n",
    "    bands_chm = chm_dataset.RasterCount\n",
    "    mapinfo_chm =chm_dataset.GetGeoTransform()\n",
    "    xMin = mapinfo_chm[0]\n",
    "    yMax = mapinfo_chm[3]\n",
    "    xMax = xMin + chm_dataset.RasterXSize/mapinfo_chm[1]\n",
    "    yMin = yMax + chm_dataset.RasterYSize/mapinfo_chm[5]\n",
    "    image_extent = (xMin,xMax,yMin,yMax)\n",
    "    \n",
    "    #Define array\n",
    "    chm_array = chm_raster.ReadAsArray(0,0,cols_chm,rows_chm).astype(np.float)\n",
    "   \n",
    "    #Smooth the CHM using a gaussian filter to remove spurious points\n",
    "    chm_array_smooth = ndi.gaussian_filter(chm_array,1,mode='constant',cval=0,truncate=2.0)\n",
    "    chm_array_smooth[chm_array==0] = 0 \n",
    "    \n",
    "    #Calculate local maximum points in the smoothed CHM\n",
    "    local_maxi = peak_local_max(chm_array_smooth,min_distance=10, threshold_abs=0, threshold_rel=0, indices=False)\n",
    "    \n",
    "    #Identify all the maximum points\n",
    "    markers = ndi.label(local_maxi)[0]\n",
    "    \n",
    "    #Create a CHM mask so the segmentation will only occur on the trees\n",
    "    chm_mask = chm_array_smooth\n",
    "    chm_mask[chm_array_smooth != 0] = 1\n",
    "    \n",
    "    #Perfrom watershed segmentation        \n",
    "    labels = watershed(chm_array_smooth, markers, mask=chm_mask)\n",
    "    \n",
    "    #Get the properties of each segment\n",
    "    tree_properties = regionprops(labels,chm_array, ['Area','BoundingBox','Centroid','Orientation','MajorAxisLength','MinorAxisLength','MaxIntensity','MinIntensity'])\n",
    "\n",
    "    #Determine how many individual trees were identified\n",
    "    max_labels = labels.max()\n",
    "    segment_labels = np.zeros(max_labels+1)\n",
    "    segment_id = np.zeros(max_labels+1)\n",
    "\n",
    "    for counter in range (1,max_labels+1):\n",
    "        segment_labels[counter] = len(labels[labels==counter])\n",
    "        segment_id[counter]=counter\n",
    "\n",
    "    #Remove the non-zero elements\n",
    "    segment_id = segment_id[np.nonzero(segment_labels)]\n",
    "    \n",
    "    #Change the labels to float\n",
    "    labels = np.array((labels),dtype=float)\n",
    "    #Change the zero labels to nans \n",
    "    labels[labels==0] = np.nan\n",
    "    \n",
    "        #Define several of the predictor variables\n",
    "    area=np.zeros(len(tree_properties))\n",
    "    diameter=np.zeros(len(tree_properties))\n",
    "    max_tree_height=np.zeros(len(tree_properties))\n",
    "    min_tree_height=np.zeros(len(tree_properties))\n",
    "    \n",
    "    #Retreive the predictor variables from the region properties\n",
    "    for counter in range(0,len(tree_properties)):\n",
    "\n",
    "        area[counter] = tree_properties[counter]['Area']\n",
    "        diameter[counter] = tree_properties[counter]['MajorAxisLength']        \n",
    "        max_tree_height[counter] =  tree_properties[counter]['MaxIntensity']    \n",
    "        min_tree_height[counter] = tree_properties[counter]['MinIntensity'] \n",
    "    \n",
    "    #Define the remaining predictor variables\n",
    "\n",
    "    crown_geometric_volume_full=np.zeros(len(segment_id))\n",
    "    crown_geometric_volume_50th_percentile=np.zeros(len(segment_id))\n",
    "    crown_geometric_volume_60th_percentile=np.zeros(len(segment_id))\n",
    "    crown_geometric_volume_70th_percentile=np.zeros(len(segment_id))\n",
    "    percentile_50th=np.zeros(len(segment_id))\n",
    "    percentile_60th=np.zeros(len(segment_id))\n",
    "    percentile_70th=np.zeros(len(segment_id))\n",
    "    \n",
    "    #Cycle through all of the tree segments    \n",
    "    counter=0\n",
    "    for segment in segment_id:\n",
    "        #Pull out the tree of interest\n",
    "        indexes_of_tree = np.asarray(np.where(labels==segment)).T\n",
    "        tree_data = chm_array[indexes_of_tree[:,0],indexes_of_tree[:,1]]\n",
    "        #Calculate the geometric volume \n",
    "        crown_geometric_volume_full[counter]=np.sum([tree_data-np.min(tree_data)])\n",
    "\n",
    "        #Pull out 50th percentile stats\n",
    "        percentile_50th[counter]=np.percentile(tree_data,50)\n",
    "        tree_data_50th = chm_array[indexes_of_tree[:,0],indexes_of_tree[:,1]]\n",
    "        tree_data_50th[tree_data_50th>percentile_50th[counter]] = percentile_50th[counter]\n",
    "        crown_geometric_volume_50th_percentile[counter]=np.sum([tree_data_50th-min_tree_height[counter]])\n",
    "\n",
    "        #Pull out 60th percentile stats    \n",
    "        percentile_60th[counter]=np.percentile(tree_data,60)\n",
    "        tree_data_60th = chm_array[indexes_of_tree[:,0],indexes_of_tree[:,1]]\n",
    "        tree_data_60th[tree_data_60th>percentile_60th[counter]] = percentile_60th[counter]\n",
    "        crown_geometric_volume_60th_percentile[counter]=np.sum([tree_data_60th-min_tree_height[counter]])\n",
    "\n",
    "        #Pull out 60th percentile stats \n",
    "        percentile_70th[counter]=np.percentile(tree_data,70)\n",
    "        tree_data_70th = chm_array[indexes_of_tree[:,0],indexes_of_tree[:,1]]\n",
    "        tree_data_70th[tree_data_70th>percentile_70th[counter]] = percentile_70th[counter]\n",
    "        crown_geometric_volume_70th_percentile[counter]=np.sum([tree_data_70th-min_tree_height[counter]])\n",
    "\n",
    "        counter=counter+1\n",
    "        \n",
    "        #Stack the predictor variables for all the individual trees\n",
    "        all_training_data = np.stack([area,diameter,max_tree_height,min_tree_height,percentile_50th,percentile_60th,percentile_70th,crown_geometric_volume_full,\n",
    "                              crown_geometric_volume_50th_percentile,crown_geometric_volume_60th_percentile,crown_geometric_volume_70th_percentile],axis=-1)\n",
    "        \n",
    "        #Apply the model to the predictive data\n",
    "        all_training_data = np.nan_to_num(all_training_data) #set nan values\n",
    "        pred_biomass = regr_rf.predict(all_training_data)\n",
    "        \n",
    "        #Set an out raster with the same size as the labels\n",
    "        biomass_out = labels\n",
    "\n",
    "        #Set counter to zero\n",
    "        counter = 0\n",
    "        #Assign each tree by the associated biomass\n",
    "        for segment in segment_id:\n",
    "            biomass_out[biomass_out==segment] = pred_biomass[counter]\n",
    "            counter = counter+1\n",
    "            \n",
    "        #Get biomass stats for plotting\n",
    "        mean_biomass = np.mean(pred_biomass)\n",
    "        std_biomass = np.std(pred_biomass)\n",
    "        min_biomass = np.min(pred_biomass)\n",
    "        sum_biomass = np.sum(pred_biomass)\n",
    "\n",
    "        #print('Sum of biomass is ',round(sum_biomass,2),' kg')     \n",
    "        return sum_biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of files to loop through \n",
    "dirpath = '/home/jovyan/NEON_CO2_Macrosystems_LIDAR/Code/Data/DP3.30015.001/DP3.30015.001/2019/FullSite/D06/2019_KONZ_4/L3/DiscreteLidar/CanopyHeightModelGtif'\n",
    "search_criteria = \"*.tif\"\n",
    "q = os.path.join(dirpath, search_criteria)\n",
    "dem_fps = glob.glob(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Timer to keep track of loop\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed7748efbf547e294f4c1a097a18b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c300d0f5d90d465982b499ac411eb40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=368.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-f3fb97897711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem_fps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-143-db8104069b7d>\u001b[0m in \u001b[0;36mopen_process\u001b[0;34m(chm_file)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m#Assign each tree by the associated biomass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msegment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msegment_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mbiomass_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbiomass_out\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_biomass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Loop through each raster in each site and add biomass\n",
    "biomass = 0\n",
    "tqdm().pandas()\n",
    "for i in tqdm(dem_fps):\n",
    "    b = open_process(i)\n",
    "    if b is None:\n",
    "        b = 0\n",
    "    biomass = biomass + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomass at the WREF site is 2674.64\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN AGAIN\n",
    "#MART_biomass = round(biomass/10**6,2)\n",
    "print(\"The biomass at the WREF site is\", MART_biomass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomass at the BONA site is 1787.57\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN AGAIN\n",
    "#BONA_biomass = round(biomass/10**6,2)\n",
    "print(\"The biomass at the BONA site is\", BONA_biomass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomass at the KONZ site is 670.55\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN AGAIN\n",
    "#KONZ_biomass = round(biomass/10**6,2)\n",
    "print(\"The biomass at the KONZ site is\", KONZ_biomass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The biomass at the NIWO site is 935.53\n"
     ]
    }
   ],
   "source": [
    "#DO NOT RUN AGAIN\n",
    "#NIWO_biomass = round(biomass/10**6,2)\n",
    "print(\"The biomass at the NIWO site is\", NIWO_biomass)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
